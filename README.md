üìö Generative AI Learning Notes ‚Äì Rohit Negi

1Ô∏è‚É£ Introduction to Generative AI(GEN AI)

=> Generative AI is a subset of artificial intelligence that creates new data/content such as text, images, music, or code based on learned patterns from training data.

2Ô∏è‚É£ Core Concepts

Tokenization
=> Breaking down input text into smaller chunks (tokens) for model processing. They are analyzing the pattern of giving input, and they convert 
or, behind the scenes, they make assumptions like the number of giving sentences.
=> Models work on token sequences instead of raw text.

Behind the scenes:

Uses Byte Pair Encoding (BPE) or SentencePiece.

Each token is mapped to an ID in a vocabulary.

GPT => Generative Pre-Trained Transform

üß† How Large Language Models (LLMs) Work Behind the Scenes

 1Ô∏è‚É£ Introduction
 => Large Language Models (LLMs) are **AI systems** trained to understand and generate human-like text.  
They work by predicting the **next token** (word, sub-word, or character) in a sequence, based on context.

2Ô∏è‚É£ Core Concepts
- **Tokens**: The smallest units of text an LLM processes.  
- **Parameters**: Model‚Äôs learned weights (e.g., GPT-3 has 175B parameters).  
- **Context Window**: The max tokens an LLM can ‚Äúremember‚Äù at once.  
- **Transformer Architecture**: The backbone neural network architecture for LLMs.

3Ô∏è‚É£ Architecture of an LLM
LLMs are mostly based on the **Transformer architecture** 

**Embedding Layer**: Converts tokens into numerical vectors.
- **Positional Encoding**: Adds sequence order information.
- **Self-Attention Mechanism**: Allows the model to focus on relevant tokens.
- **Feed-Forward Neural Networks**: Processes token representations.
- **Layer Normalization & Residual Connections**: Helps training stability.
- **Output Layer**: Predicts the next token probabilities.

Input Text ‚Üí Tokenization ‚Üí Embeddings + Positional Encoding ‚Üí Multi-Head Self Attention ‚Üí Feed Forward Layers ‚Üí Output Prediction

1Ô∏è‚É£ Pure LLM
The model relies only on its trained parameters (the neural network weights) to answer.

No internet, no API calls, no extra databases ‚Äî only what it learned during training.

2Ô∏è‚É£ External Tool
The LLM connects to external systems to enhance its capabilities.

It acts as the ‚Äúcontroller,‚Äù deciding when to call APIs, search the web, run code, or query databases.

Examples of external tools:

Web Search API ‚Üí Fetch latest news.

Calculator / Code Interpreter ‚Üí Perform exact math or code execution.

Database Query ‚Üí Retrieve structured data.

Custom API ‚Üí Weather, stock prices, translation, etc.

